"""
Shared file commands for CLI and Dashboard.

Extracted from cli.py to enable reuse across interfaces.
All commands accept OutputAdapter for UI-agnostic rendering.
"""

import glob as glob_module
import itertools
import time
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    from lobster.core.client import AgentClient

from lobster.cli_internal.commands.output_adapter import OutputAdapter
from lobster.cli_internal.utils.path_resolution import PathResolver
from lobster.core.component_registry import component_registry

# Import extraction cache manager (premium feature - graceful fallback if unavailable)
ExtractionCacheManager = component_registry.get_service('extraction_cache')
HAS_EXTRACTION_CACHE = ExtractionCacheManager is not None


def file_read(
    client: "AgentClient",
    output: OutputAdapter,
    filename: str,
    current_directory: Path,
    path_resolver_class=PathResolver
) -> Optional[str]:
    """
    Read workspace files with multiple format support.

    Handles text files, code files, data files, archives, and glob patterns.
    For non-text files, provides file info and guidance on how to load them.

    Args:
        client: AgentClient instance
        output: OutputAdapter for rendering
        filename: File path or glob pattern to read
        current_directory: Current working directory
        path_resolver_class: PathResolver class for secure path resolution (default: PathResolver)

    Returns:
        Summary string for conversation history, or None

    Features:
        - Glob pattern support (*.py, data/*.csv, etc.)
        - Syntax highlighting for code files
        - Security via PathResolver
        - Binary file detection
        - File size limits (10MB for text display)
        - Suggestions for loading data files
    """
    if not filename:
        output.print("[yellow]Usage: /read <file|pattern>[/yellow]", style="warning")
        output.print("[grey50]  View file contents (text files only)[/grey50]", style="info")
        output.print("[grey50]  Use /workspace load <file> to load data files[/grey50]", style="info")
        return None

    # Check if filename contains glob patterns (before path resolution)
    is_glob_pattern = any(char in filename for char in ["*", "?", "[", "]"])

    # BUG FIX #6: Use PathResolver for secure path resolution (non-glob paths)
    if not is_glob_pattern:
        resolver = path_resolver_class(
            current_directory=current_directory,
            workspace_path=(
                client.data_manager.workspace_path
                if hasattr(client, "data_manager")
                else None
            ),
        )
        resolved = resolver.resolve(
            filename, search_workspace=True, must_exist=False
        )

        if not resolved.is_safe:
            output.print(f"[red]‚ùå Security error: {resolved.error}[/red]", style="error")
            return None

        file_path = resolved.path
    else:
        # Glob patterns need special handling - construct search pattern
        file_path = (
            Path(filename)
            if Path(filename).is_absolute()
            else current_directory / filename
        )

    if is_glob_pattern:
        # Handle glob patterns - show contents of matching text files
        if not Path(filename).is_absolute():
            base_path = current_directory
            search_pattern = str(base_path / filename)
        else:
            search_pattern = filename

        # BUG FIX #3: Use lazy evaluation to prevent memory explosion
        # Only load first 10 file paths instead of all matches
        matching_files = list(
            itertools.islice(glob_module.iglob(search_pattern), 10)
        )

        if not matching_files:
            output.print(
                f"[red]‚ùå No files found matching pattern: {filename}[/red]",
                style="error"
            )
            output.print(f"[grey50]Searched in: {current_directory}[/grey50]", style="info")
            return None

        # Count total matches without loading all paths
        total_count = sum(1 for _ in glob_module.iglob(search_pattern))

        matching_files.sort()
        output.print(
            f"[cyan]üìÅ Found {total_count} files matching '[white]{filename}[/white]', displaying first 10[/cyan]\n",
            style="info"
        )

        displayed_count = 0
        for match_path in matching_files:  # Already limited to 10
            match_file = Path(match_path)
            file_info = client.detect_file_type(match_file)

            # Only display text files
            if not file_info.get("binary", True):
                try:
                    # BUG FIX #3: Add file size check before reading (10MB limit)
                    file_size = match_file.stat().st_size
                    if file_size > 10_000_000:  # 10MB
                        output.print(
                            f"[yellow]‚ö†Ô∏è  {match_file.name} too large to display ({file_size / 1_000_000:.1f}MB, limit: 10MB)[/yellow]",
                            style="warning"
                        )
                        continue

                    content = match_file.read_text(encoding="utf-8")
                    lines = content.splitlines()

                    # Language detection
                    ext = match_file.suffix.lower()
                    language_map = {
                        ".py": "python",
                        ".js": "javascript",
                        ".ts": "typescript",
                        ".html": "html",
                        ".css": "css",
                        ".json": "json",
                        ".xml": "xml",
                        ".yaml": "yaml",
                        ".yml": "yaml",
                        ".sh": "bash",
                        ".bash": "bash",
                        ".md": "markdown",
                        ".txt": "text",
                        ".log": "text",
                        ".r": "r",
                        ".csv": "csv",
                        ".tsv": "csv",
                        ".ris": "text",
                    }
                    language = language_map.get(ext, "text")

                    # Display using code block
                    output.print(f"\n[cyan]üìÑ {match_file.name}[/cyan] [grey50]({len(lines)} lines)[/grey50]", style="info")
                    output.print_code_block(content, language=language)
                    displayed_count += 1
                except Exception as e:
                    output.print(
                        f"[yellow]‚ö†Ô∏è  Could not read {match_file.name}: {e}[/yellow]",
                        style="warning"
                    )
            else:
                output.print(
                    f"[grey50]  ‚Ä¢ {match_file.name} (binary file - skipped)[/grey50]",
                    style="info"
                )

        if total_count > 10:
            output.print(
                f"\n[grey50]... and {total_count - 10} more files (not loaded)[/grey50]",
                style="info"
            )

        return f"Displayed {displayed_count} text files matching '{filename}' (total: {total_count})"

    # Single file processing
    if not file_path.exists():
        # Try to locate via client (searches workspace directories)
        file_info = client.locate_file(filename)
        if not file_info["found"]:
            output.print(
                f"[bold red on white] ‚ö†Ô∏è  Error [/bold red on white] [red]{file_info['error']}[/red]",
                style="error"
            )
            if "searched_paths" in file_info:
                output.print("[grey50]Searched in:[/grey50]", style="info")
                for path in file_info["searched_paths"][:5]:
                    output.print(f"  ‚Ä¢ [grey50]{path}[/grey50]", style="info")
            return f"File '{filename}' not found"
        file_path = file_info["path"]

    # Get file info
    file_info = client.detect_file_type(file_path)
    file_description = file_info.get("description", "Unknown")
    file_category = file_info.get("category", "unknown")
    is_binary = file_info.get("binary", True)

    # Show file location
    output.print(f"[cyan]üìÑ File:[/cyan] [white]{file_path.name}[/white]", style="info")
    output.print(f"[grey50]   Path: {file_path}[/grey50]", style="info")
    output.print(f"[grey50]   Type: {file_description}[/grey50]", style="info")

    # Handle text files - display content
    if not is_binary:
        try:
            content = file_path.read_text(encoding="utf-8")
            lines = content.splitlines()

            # Language detection
            ext = file_path.suffix.lower()
            language_map = {
                ".py": "python",
                ".js": "javascript",
                ".ts": "typescript",
                ".html": "html",
                ".css": "css",
                ".json": "json",
                ".xml": "xml",
                ".yaml": "yaml",
                ".yml": "yaml",
                ".sh": "bash",
                ".bash": "bash",
                ".md": "markdown",
                ".txt": "text",
                ".log": "text",
                ".r": "r",
                ".csv": "csv",
                ".tsv": "csv",
                ".ris": "text",
            }
            language = language_map.get(ext, "text")

            # Display using code block
            output.print(f"\n[bold red]üìÑ {file_path.name}[/bold red]", style="info")
            output.print_code_block(content, language=language)

            return f"Displayed text file '{filename}' ({file_description}, {len(lines)} lines)"

        except UnicodeDecodeError:
            output.print(
                "[yellow]‚ö†Ô∏è  File appears to be binary despite extension[/yellow]",
                style="warning"
            )
            is_binary = True
        except Exception as e:
            output.print(f"[red]Error reading file: {e}[/red]", style="error")
            return f"Error reading file '{filename}': {str(e)}"

    # Handle binary/data files - show info only, suggest /workspace load
    if is_binary:
        output.print(
            "\n[bold yellow on black] ‚ÑπÔ∏è  File Info [/bold yellow on black]",
            style="info"
        )

        # Format file size
        size_bytes = file_path.stat().st_size
        if size_bytes < 1024:
            size_str = f"{size_bytes} bytes"
        elif size_bytes < 1024**2:
            size_str = f"{size_bytes/1024:.1f} KB"
        elif size_bytes < 1024**3:
            size_str = f"{size_bytes/1024**2:.1f} MB"
        else:
            size_str = f"{size_bytes/1024**3:.1f} GB"

        output.print(f"[white]Size: [yellow]{size_str}[/yellow][/white]", style="info")

        # Provide guidance based on file type
        if file_category == "bioinformatics":
            output.print(
                f"\n[cyan]üí° This is a bioinformatics data file ({file_description}).[/cyan]",
                style="info"
            )
            output.print(
                f"[cyan]   To load into workspace: [yellow]/workspace load {filename}[/yellow][/cyan]",
                style="info"
            )
        elif file_category == "tabular":
            output.print(
                f"\n[cyan]üí° This is a tabular data file ({file_description}).[/cyan]",
                style="info"
            )
            output.print(
                f"[cyan]   To load into workspace: [yellow]/workspace load {filename}[/yellow][/cyan]",
                style="info"
            )
        elif file_category == "archive":
            output.print(
                f"\n[cyan]üí° This is an archive file ({file_description}).[/cyan]",
                style="info"
            )
            output.print(
                f"[cyan]   To extract and load: [yellow]/workspace load {filename}[/yellow][/cyan]",
                style="info"
            )
        elif file_category == "image":
            output.print(
                "[cyan]üí° This is an image file. Use your system's image viewer to open it.[/cyan]",
                style="info"
            )
        else:
            output.print(
                "[cyan]üí° Binary file - use external tools to view.[/cyan]",
                style="info"
            )

        return f"Inspected file '{filename}' ({file_description}, {size_str}) - use /workspace load to load data files"


def archive_queue(
    client: "AgentClient",
    output: OutputAdapter,
    subcommand: str = "help",
    args: Optional[str] = None
) -> Optional[str]:
    """
    Archive queue functionality for cached extractions.

    Manages extraction cache for nested archive files, allowing users to
    list, load, and inspect cached archive contents without re-extracting.

    Args:
        client: AgentClient instance
        output: OutputAdapter for rendering
        subcommand: Archive subcommand (list, groups, load, status, cleanup, help)
        args: Additional arguments for subcommands (e.g., pattern for load, limit flags)

    Returns:
        Summary string for conversation history, or None

    Subcommands:
        - list: Show all samples in cached archive
        - groups: Show condition groups summary
        - load <pattern>: Load samples by pattern/GSM ID/condition
        - status: Show extraction cache status
        - cleanup: Clear old cached extractions (>7 days)
        - help: Show usage information

    Premium Feature:
        Archive caching requires premium distribution.
    """
    # BUG FIX #1: Handle nested archive commands with proper cache management
    # Use ExtractionCacheManager instead of client instance variables to prevent race conditions

    # Check if extraction cache is available (premium feature)
    if not HAS_EXTRACTION_CACHE:
        output.print(
            "[yellow]Archive caching is a premium feature not available in this distribution.[/yellow]",
            style="warning"
        )
        output.print(
            "[dim]Use extract_and_load_archive() to load entire archives instead.[/dim]",
            style="info"
        )
        return None

    # Initialize cache manager (thread-safe, per-request instance)
    cache_manager = ExtractionCacheManager(client.data_manager.workspace_path)
    recent_caches = cache_manager.list_all_caches()

    if not recent_caches:
        output.print("[red]‚ùå No cached archives found[/red]", style="error")
        output.print(
            "[yellow]üí° Run /read <archive.tar> first to inspect an archive[/yellow]",
            style="warning"
        )
        return None

    # Select cache: use most recent if only one, otherwise prompt user
    if len(recent_caches) == 1:
        cache_id = recent_caches[0]["cache_id"]
    else:
        # Multiple caches available - show list and use most recent
        output.print(
            f"\n[cyan]üì¶ Found {len(recent_caches)} cached archives (using most recent):[/cyan]",
            style="info"
        )
        for i, cache in enumerate(recent_caches[:3], 1):
            age_hours = (time.time() - cache.get("timestamp", 0)) / 3600
            output.print(f"  {i}. {cache['cache_id']} ({age_hours:.1f}h ago)", style="info")
        cache_id = recent_caches[0]["cache_id"]  # Most recent

    # Get cache info and nested_info
    cache_info = cache_manager.get_cache_info(cache_id)
    if not cache_info:
        output.print(f"[red]‚ùå Cache {cache_id} metadata not found[/red]", style="error")
        return None

    nested_info = cache_info.get("nested_info")
    if not nested_info:
        output.print(
            f"[red]‚ùå Cache {cache_id} missing nested structure info[/red]",
            style="error"
        )
        return None

    if subcommand == "list":
        # Show detailed list of all nested samples
        output.print("\n[bold white]üìã Archive Contents:[/bold white]", style="info")
        output.print(f"[dim]Cache ID: {cache_id}[/dim]\n", style="info")

        samples_table_data = {
            "title": "All Samples",
            "columns": [
                {"name": "GSM ID", "style": "bold orange1"},
                {"name": "Condition", "style": "white"},
                {"name": "Number", "style": "grey70"},
                {"name": "Filename", "style": "dim"},
            ],
            "rows": []
        }

        for condition, samples in nested_info.groups.items():
            for sample in samples:
                samples_table_data["rows"].append([
                    sample["gsm_id"],
                    condition,
                    sample["number"],
                    sample["filename"],
                ])

        output.print_table(samples_table_data)
        return f"Listed {nested_info.total_count} samples from cached archive"

    elif subcommand == "groups":
        # Show condition groups summary (nested_info already loaded above)
        output.print("\n[bold white]üìÇ Condition Groups:[/bold white]\n", style="info")

        groups_table_data = {
            "title": None,
            "columns": [
                {"name": "Condition", "style": "bold orange1"},
                {"name": "Sample Count", "style": "white"},
                {"name": "GSM IDs", "style": "grey70"},
            ],
            "rows": []
        }

        for condition, samples in nested_info.groups.items():
            gsm_ids = [s["gsm_id"] for s in samples]
            groups_table_data["rows"].append([
                condition,
                str(len(samples)),
                f"{min(gsm_ids)}-{max(gsm_ids)}" if gsm_ids else "N/A",
            ])

        output.print_table(groups_table_data)
        return f"Displayed {len(nested_info.groups)} condition groups"

    elif subcommand == "load":
        # Load samples by pattern
        if not args:
            output.print(
                "[yellow]Usage: /archive load <pattern|GSM_ID|condition>[/yellow]",
                style="warning"
            )
            output.print("[dim]Examples:[/dim]", style="info")
            output.print("[dim]  /archive load GSM4710689[/dim]", style="info")
            output.print("[dim]  /archive load TISSUE[/dim]", style="info")
            output.print("[dim]  /archive load PDAC_* --limit 3[/dim]", style="info")
            return None

        pattern_arg = args

        # Parse limit flag
        limit = None
        if "--limit" in pattern_arg:
            pattern, limit_str = pattern_arg.split("--limit")
            pattern = pattern.strip()
            try:
                limit = int(limit_str.strip())
            except ValueError:
                output.print("[red]‚ùå Invalid limit value[/red]", style="error")
                return None
        else:
            pattern = pattern_arg

        output.print(
            f"[cyan]üîÑ Loading samples matching '[bold]{pattern}[/bold]'...[/cyan]",
            style="info"
        )

        # Note: Using status context is not supported by OutputAdapter abstraction
        # Real implementation would need client.load_from_cache to handle this
        result = client.load_from_cache(cache_id, pattern, limit)

        if result["success"]:
            output.print(f"\n[green]‚úì {result['message']}[/green]", style="success")

            # Display merged dataset if auto-concatenation occurred
            if "merged_modality" in result:
                merged_name = result["merged_modality"]

                # Get merged dataset details
                try:
                    merged_adata = client.data_manager.get_modality(merged_name)

                    # Create prominent merged dataset panel info
                    merged_info = f"""[bold white]Merged Dataset:[/bold white] [orange1]{merged_name}[/orange1]

[white]Shape:[/white] [cyan]{merged_adata.n_obs:,} cells √ó {merged_adata.n_vars:,} genes[/cyan]
[white]Batches:[/white] [cyan]{result['loaded_count']} samples merged[/cyan]
[white]Batch key:[/white] [cyan]sample_id[/cyan]

[bold white]üéØ Ready for Analysis![/bold white]
[grey70]  ‚Ä¢ Say: "Show me a UMAP of this dataset"[/grey70]
[grey70]  ‚Ä¢ Say: "Perform quality control"[/grey70]
[grey70]  ‚Ä¢ Or use: /data to inspect the dataset[/grey70]"""

                    output.print(
                        f"\n[bold green]‚ú® Auto-Merged Dataset[/bold green]\n{merged_info}",
                        style="success"
                    )

                except Exception as e:
                    output.print(
                        f"\n[yellow]‚ö†Ô∏è  Could not display merged dataset details: {e}[/yellow]",
                        style="warning"
                    )

                # Show individual modalities in collapsed format
                output.print(
                    f"\n[dim]Individual modalities (merged into '{merged_name}'):[/dim]",
                    style="info"
                )
                for i, modality in enumerate(result["modalities"][:5], 1):
                    output.print(f"  [dim]{i}. {modality}[/dim]", style="info")
                if len(result["modalities"]) > 5:
                    output.print(
                        f"  [dim]... and {len(result['modalities'])-5} more[/dim]",
                        style="info"
                    )

            else:
                # Single sample or no auto-concatenation
                output.print("\n[bold white]Loaded Modalities:[/bold white]", style="info")
                for modality in result["modalities"]:
                    output.print(f"  ‚Ä¢ [cyan]{modality}[/cyan]", style="info")

                # Suggest next steps
                output.print("\n[bold white]üéØ Next Steps:[/bold white]", style="info")
                output.print(
                    "[grey70]  ‚Ä¢ Use /data to inspect the dataset[/grey70]",
                    style="info"
                )
                output.print(
                    "[grey70]  ‚Ä¢ Say: 'Analyze this dataset' for natural language analysis[/grey70]",
                    style="info"
                )

            if result["failed"]:
                output.print(
                    f"\n[yellow]‚ö†Ô∏è  Failed to load {len(result['failed'])} samples:[/yellow]",
                    style="warning"
                )
                for failed in result["failed"][:5]:
                    output.print(f"  ‚Ä¢ [dim]{failed}[/dim]", style="info")
                if len(result["failed"]) > 5:
                    output.print(
                        f"  ‚Ä¢ [dim]... and {len(result['failed'])-5} more[/dim]",
                        style="info"
                    )

            # Return summary
            if "merged_modality" in result:
                return f"Merged {result['loaded_count']} samples into '{result['merged_modality']}'"
            else:
                return f"Loaded {result['loaded_count']} samples: {', '.join(result['modalities'][:3])}{'...' if len(result['modalities']) > 3 else ''}"

        else:
            output.print(f"\n[red]‚ùå {result['error']}[/red]", style="error")
            if "suggestion" in result:
                output.print(f"[yellow]üí° {result['suggestion']}[/yellow]", style="warning")
            return f"Failed to load samples: {result['error']}"

    elif subcommand == "status":
        # Show cache status (uses top-level import, already checked HAS_EXTRACTION_CACHE)
        cache_manager = ExtractionCacheManager(client.workspace_path)
        all_caches = cache_manager.list_all_caches()

        output.print("\n[bold white]üìä Extraction Cache Status:[/bold white]\n", style="info")
        output.print(
            f"[white]Total cached extractions: [yellow]{len(all_caches)}[/yellow][/white]",
            style="info"
        )

        if all_caches:
            cache_table_data = {
                "title": None,
                "columns": [
                    {"name": "Cache ID", "style": "bold orange1"},
                    {"name": "Archive", "style": "white"},
                    {"name": "Samples", "style": "yellow"},
                    {"name": "Extracted At", "style": "dim"},
                ],
                "rows": []
            }

            for cache in all_caches:
                extracted_at = datetime.fromisoformat(cache["extracted_at"])
                cache_table_data["rows"].append([
                    cache["cache_id"],
                    Path(cache["archive_path"]).name,
                    str(cache["nested_info"]["total_count"]),
                    extracted_at.strftime("%Y-%m-%d %H:%M"),
                ])

            output.print_table(cache_table_data)

        return f"Cache status: {len(all_caches)} active extractions"

    elif subcommand == "cleanup":
        # Clean up old caches (uses top-level import, already checked HAS_EXTRACTION_CACHE)
        cache_manager = ExtractionCacheManager(client.workspace_path)

        output.print("[cyan]üßπ Cleaning up old cached extractions...[/cyan]", style="info")
        removed_count = cache_manager.cleanup_old_caches(max_age_days=7)

        output.print(f"[green]‚úì Removed {removed_count} old cache(s)[/green]", style="success")
        return f"Cleaned up {removed_count} old cached extractions"

    else:
        # Show help
        output.print("\n[bold white]üì¶ /archive Commands:[/bold white]\n", style="info")
        output.print(
            "[orange1]/archive list[/orange1]             - List all samples in inspected archive",
            style="info"
        )
        output.print(
            "[orange1]/archive groups[/orange1]           - Show condition groups",
            style="info"
        )
        output.print(
            "[orange1]/archive load <pattern>[/orange1]   - Load samples by pattern",
            style="info"
        )
        output.print(
            "[orange1]/archive status[/orange1]           - Show extraction cache status",
            style="info"
        )
        output.print(
            "[orange1]/archive cleanup[/orange1]          - Clear old cached extractions\n",
            style="info"
        )

        output.print("[bold white]Loading Patterns:[/bold white]", style="info")
        output.print("[grey70]‚Ä¢ GSM ID:[/grey70]        GSM4710689", style="info")
        output.print("[grey70]‚Ä¢ Condition:[/grey70]     TISSUE, PDAC_TISSUE", style="info")
        output.print("[grey70]‚Ä¢ Glob:[/grey70]          PDAC_*, *_TISSUE_*", style="info")
        output.print("[grey70]‚Ä¢ With limit:[/grey70]    TISSUE --limit 3", style="info")

        return None
