"""
Jupyter Notebook Exporter for Pipeline Replay.

This module converts Lobster provenance records into executable Jupyter notebooks,
enabling reproducible analysis workflows using industry-standard tools.

The exporter uses Service-Emitted Intermediate Representation (IR) to automatically
generate code without manual mapping registries, achieving 95%+ executable notebooks.
"""

import hashlib
import json
import logging
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import nbformat
from nbformat import NotebookNode
from nbformat.v4 import new_code_cell, new_markdown_cell, new_notebook

from lobster.core.analysis_ir import AnalysisStep, extract_unique_imports
from lobster.core.data_manager_v2 import DataManagerV2
from lobster.core.provenance import ProvenanceTracker

logger = logging.getLogger(__name__)


class NotebookExporter:
    """
    Convert Lobster provenance to executable Jupyter notebook using IR.

    This class transforms provenance tracking data with embedded Intermediate
    Representation (IR) into reproducible analysis pipelines using standard
    Jupyter notebook format compatible with Papermill for parameterized execution.

    The key innovation is that services emit AnalysisStep IR objects that contain
    complete code generation instructions, eliminating the need for manual mapping
    registries like TOOL_TO_CODE.

    Attributes:
        provenance: ProvenanceTracker instance with analysis history
        data_manager: DataManagerV2 instance for data context
    """

    def __init__(
        self, provenance: ProvenanceTracker, data_manager: DataManagerV2
    ) -> None:
        """
        Initialize notebook exporter.

        Args:
            provenance: ProvenanceTracker with recorded activities
            data_manager: DataManagerV2 for workspace context
        """
        self.provenance = provenance
        self.data_manager = data_manager
        logger.debug("Initialized NotebookExporter with IR support")

    def export(
        self,
        name: str,
        description: str = "",
        filter_strategy: str = "successful",
        validate_syntax: bool = True,
    ) -> Path:
        """
        Generate Jupyter notebook from current session using IR.

        Args:
            name: Notebook filename (without extension)
            description: Human-readable description for header
            filter_strategy: Activity filter ("successful" | "all" | "manual")
            validate_syntax: Whether to validate generated Python syntax

        Returns:
            Path to generated .ipynb file

        Raises:
            ValueError: If no activities to export or name is invalid
            RuntimeError: If IR extraction fails or code generation invalid
        """
        if not name or not name.strip():
            raise ValueError("Notebook name cannot be empty")

        if not self.provenance.activities:
            raise ValueError("No activities recorded - nothing to export")

        logger.info(f"Exporting notebook with IR: {name}")

        # Filter activities by success status
        activities = self._filter_activities(filter_strategy)
        logger.debug(f"Filtered {len(activities)} activities for export")

        # Extract only exportable (activity, IR) pairs
        # This filters out orchestration activities (publication processing, etc.)
        # that belong in provenance but not in executable notebooks
        exportable_pairs = self._get_exportable_activity_ir_pairs(activities)
        exportable_count = len(exportable_pairs)
        filtered_count = len(activities) - exportable_count

        logger.info(
            f"Extracted {exportable_count} exportable IR objects "
            f"from {len(activities)} activities "
            f"({filtered_count} provenance-only activities filtered)"
        )

        # Extract IRs for imports and parameters collection
        irs = [ir for _, ir in exportable_pairs]

        if exportable_count == 0:
            raise ValueError(
                f"No exportable IR objects found in provenance. "
                f"Found {len(activities)} activities, but none have exportable=True IR. "
                f"Services need to emit AnalysisStep objects with exportable=True."
            )

        # Create new notebook
        notebook = new_notebook()

        # Add header
        notebook.cells.append(
            self._create_header_cell(name, description, exportable_count)
        )

        # Add data integrity manifest (cryptographic hashes for compliance)
        notebook.cells.append(self._create_integrity_manifest_cell())

        # Collect and add imports from all exportable IRs
        imports_cell = self._create_imports_cell(irs)
        notebook.cells.append(imports_cell)

        # Add parameters cell (Papermill-tagged)
        notebook.cells.append(self._create_parameters_cell(irs))

        # Add data loading cell (critical for execution)
        notebook.cells.append(self._create_data_loading_cell())

        # Convert exportable (activity, IR) pairs to code cells
        # This iterates over ONLY exportable IRs, not all activities
        for idx, (activity, ir) in enumerate(exportable_pairs, start=1):
            # Add documentation cell
            notebook.cells.append(self._create_doc_cell(activity, idx))

            # Add executable code cell from IR (no placeholder fallback)
            code_cell = self._ir_to_code_cell(ir, validate=validate_syntax)
            notebook.cells.append(code_cell)

        # Add data saving cell
        notebook.cells.append(self._create_data_saving_cell())

        # Add provenance summary cell (explains what's in notebook vs provenance)
        notebook.cells.append(
            self._create_provenance_summary_cell(len(activities), exportable_count)
        )

        # Add footer cell
        notebook.cells.append(self._create_footer_cell())

        # Add notebook metadata
        notebook.metadata["lobster"] = self._create_metadata(
            exportable_count, len(activities)
        )

        # Save to workspace notebooks directory
        output_dir = self.data_manager.workspace_path / "notebooks"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"{name}.ipynb"

        with open(output_path, "w") as f:
            nbformat.write(notebook, f)

        logger.info(f"Notebook exported: {output_path}")
        logger.info(
            f"Executable steps: {exportable_count} | "
            f"Provenance-only: {filtered_count} | "
            f"Total activities: {len(activities)}"
        )

        return output_path

    def _filter_activities(self, strategy: str) -> List[Dict[str, Any]]:
        """
        Filter provenance activities based on strategy.

        Args:
            strategy: Filter strategy ("successful" | "all" | "manual")

        Returns:
            List of filtered activity dictionaries
        """
        if strategy == "successful":
            # Exclude activities with error markers
            return [
                a
                for a in self.provenance.activities
                if not a.get("error") and a.get("type") != "failed_operation"
            ]
        elif strategy == "all":
            return self.provenance.activities.copy()
        else:
            # Manual filtering not implemented yet
            logger.warning("Manual filter strategy not implemented, using 'successful'")
            return self._filter_activities("successful")

    def _extract_irs(self, activities: List[Dict[str, Any]]) -> List[AnalysisStep]:
        """
        Extract AnalysisStep IR objects from provenance activities.

        Args:
            activities: List of activity dictionaries from provenance

        Returns:
            List of AnalysisStep objects (may be empty if no IRs present)
        """
        irs = []

        for activity in activities:
            ir_dict = activity.get("ir")

            if ir_dict is not None and isinstance(ir_dict, dict):
                try:
                    ir = AnalysisStep.from_dict(ir_dict)
                    # Check if this IR should be included in notebook export
                    if ir.exportable:
                        irs.append(ir)
                        logger.debug(f"Extracted IR for operation: {ir.operation}")
                    else:
                        logger.debug(
                            f"Skipping non-exportable IR for operation: {ir.operation}"
                        )
                except Exception as e:
                    logger.warning(
                        f"Failed to deserialize IR for activity "
                        f"{activity.get('type')}: {e}"
                    )

        return irs

    def _get_exportable_activity_ir_pairs(
        self, activities: List[Dict[str, Any]]
    ) -> List[Tuple[Dict[str, Any], AnalysisStep]]:
        """
        Extract (activity, IR) pairs where IR exists and is exportable.

        Filters out orchestration activities (e.g., publication processing,
        metadata extraction) that log provenance but don't need notebook
        representation. This ensures notebooks contain only executable
        analysis steps while maintaining complete audit trail in provenance.

        Args:
            activities: List of activity dictionaries from provenance

        Returns:
            List of (activity, AnalysisStep) tuples for exportable steps only
        """
        pairs = []

        for activity in activities:
            ir_dict = activity.get("ir")

            if ir_dict is not None and isinstance(ir_dict, dict):
                try:
                    ir = AnalysisStep.from_dict(ir_dict)
                    if ir.exportable:
                        pairs.append((activity, ir))
                        logger.debug(f"Included exportable IR: {ir.operation}")
                    else:
                        logger.debug(f"Filtered non-exportable IR: {ir.operation}")
                except Exception as e:
                    logger.warning(
                        f"Failed to deserialize IR for activity "
                        f"{activity.get('type')}: {e}"
                    )
            else:
                # Activity has no IR - provenance-only (e.g., orchestration)
                logger.debug(
                    f"Filtered provenance-only activity: {activity.get('type')}"
                )

        return pairs

    def _create_header_cell(
        self, name: str, description: str, n_irs: int
    ) -> NotebookNode:
        """
        Create markdown header cell for notebook.

        Args:
            name: Notebook name
            description: Optional description
            n_irs: Number of IR objects extracted

        Returns:
            Markdown cell with header content
        """
        coverage_pct = (
            n_irs / len(self.provenance.activities) * 100
            if self.provenance.activities
            else 0
        )

        header_content = f"""# {name}

**Generated from Lobster AI Session**

{description if description else "Reproducible bioinformatics analysis workflow"}

**Created:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Lobster Version:** {self._get_lobster_version()}
**IR Coverage:** {n_irs}/{len(self.provenance.activities)} activities ({coverage_pct:.1f}%)

---

## Workflow Summary

This notebook was automatically generated from a Lobster analysis session using
Service-Emitted Intermediate Representation (IR). Each analysis step contains
complete code generation instructions for reproducibility.

**Session Statistics:**
- Activities: {len(self.provenance.activities)}
- Entities: {len(self.provenance.entities)}
- Data Modalities: {len(self.data_manager.modalities)}
- IR Objects: {n_irs}

---
"""
        return new_markdown_cell(header_content)

    def _create_integrity_manifest_cell(self) -> NotebookNode:
        """
        Create data integrity manifest cell with cryptographic hashes.

        This cell provides ALCOA+ compliant data integrity verification by
        recording SHA-256 hashes of all inputs, provenance, and system state.
        Critical for 21 CFR Part 11 and GxP compliance.

        Returns:
            Markdown cell with integrity manifest
        """
        manifest = {
            "data_integrity_manifest": {
                "generated_at": datetime.now().isoformat(),
                "provenance": self._get_provenance_hash(),
                "input_files": self._get_input_file_hashes(),
                "system": self._get_system_info(),
            }
        }

        content = f"""## ðŸ”’ Data Integrity Manifest

**Purpose**: Cryptographic verification of data integrity (ALCOA+ compliance)

```json
{json.dumps(manifest, indent=2)}
```

**Verification**: The hashes above provide tamper-evident proof that:
- This analysis used the exact input data listed
- The provenance record matches this session
- The system version is documented and reproducible

âš ï¸ **Critical**: Any modification to input data will result in hash mismatch, invalidating the analysis.

---
"""
        return new_markdown_cell(content)

    def _get_provenance_hash(self) -> Dict[str, str]:
        """Calculate SHA-256 hash of provenance JSON."""
        try:
            # Serialize provenance to consistent JSON
            prov_data = {
                "namespace": self.provenance.namespace,
                "activities_count": len(self.provenance.activities),
                "entities_count": len(self.provenance.entities),
            }
            prov_json = json.dumps(prov_data, sort_keys=True).encode("utf-8")
            prov_hash = hashlib.sha256(prov_json).hexdigest()

            return {
                "session_id": self.provenance.namespace,
                "sha256": prov_hash,
                "activities": len(self.provenance.activities),
                "entities": len(self.provenance.entities),
            }
        except Exception as e:
            logger.warning(f"Failed to hash provenance: {e}")
            return {"session_id": self.provenance.namespace, "sha256": "unavailable"}

    def _get_input_file_hashes(self) -> Dict[str, str]:
        """Calculate SHA-256 hashes of input data files."""
        file_hashes = {}

        try:
            # Hash modality files from workspace
            for modality_name in self.data_manager.modalities:
                modality_path = (
                    self.data_manager.workspace_path / f"{modality_name}.h5ad"
                )
                if modality_path.exists():
                    file_hash = self._calculate_file_hash(modality_path)
                    file_hashes[f"{modality_name}.h5ad"] = file_hash
                else:
                    file_hashes[f"{modality_name}.h5ad"] = "file_not_found"

        except Exception as e:
            logger.warning(f"Failed to hash input files: {e}")
            file_hashes["error"] = str(e)

        return file_hashes if file_hashes else {"note": "No input files hashed"}

    def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA-256 hash of a file."""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                # Read file in chunks for memory efficiency
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.error(f"Failed to hash {file_path}: {e}")
            return "hash_failed"

    def _get_system_info(self) -> Dict[str, str]:
        """Get system information including Lobster version and Git commit."""
        system_info = {
            "lobster_version": self._get_lobster_version(),
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            "platform": sys.platform,
        }

        # Try to get Git commit hash
        try:
            git_commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL
            ).decode("utf-8").strip()
            system_info["git_commit"] = git_commit[:8]  # Short hash
        except Exception:
            system_info["git_commit"] = "unavailable"

        return system_info

    def _create_imports_cell(self, irs: List[AnalysisStep]) -> NotebookNode:
        """
        Create imports cell with deduplicated imports from all IRs.

        Args:
            irs: List of AnalysisStep objects

        Returns:
            Code cell with all required imports
        """
        if not irs:
            # Default imports if no IRs present
            imports = [
                "import numpy as np",
                "import pandas as pd",
                "import scanpy as sc",
                "import anndata",
            ]
        else:
            imports = extract_unique_imports(irs)

        imports_code = "# Required imports\n" + "\n".join(imports)

        logger.debug(f"Generated {len(imports)} import statements")

        return new_code_cell(imports_code)

    def _create_parameters_cell(self, irs: List[AnalysisStep]) -> NotebookNode:
        """
        Create Papermill parameters cell dynamically from IR schemas.

        Args:
            irs: List of AnalysisStep objects

        Returns:
            Code cell tagged for Papermill parameter injection
        """
        # Collect all Papermill-injectable parameters from IRs
        injectable_params: Dict[str, Any] = {}

        for ir in irs:
            papermill_params = ir.get_papermill_parameters()
            injectable_params.update(papermill_params)

        # Standard parameters
        code = """# Parameters (tagged for Papermill parameter injection)
# Override with: papermill notebook.ipynb output.ipynb -p param_name value

# Input/Output paths
input_data = "dataset.h5ad"  # Path to input H5AD file
output_prefix = "results"     # Prefix for output files

# Reproducibility
random_seed = 42              # Random seed for reproducibility
"""

        # Add IR-derived parameters if present
        if injectable_params:
            code += "\n# Analysis parameters (from workflow IR)\n"
            for param_name, param_value in sorted(injectable_params.items()):
                # Format value for Python
                if isinstance(param_value, str):
                    value_str = f'"{param_value}"'
                elif isinstance(param_value, (list, tuple)):
                    value_str = repr(param_value)
                else:
                    value_str = str(param_value)

                code += f"{param_name} = {value_str}\n"

        cell = new_code_cell(code)
        cell.metadata["tags"] = ["parameters"]

        logger.debug(
            f"Generated parameters cell with {len(injectable_params)} "
            f"IR-derived parameters"
        )

        return cell

    def _create_data_loading_cell(self) -> NotebookNode:
        """
        Create cell for loading input data using IR if available.

        Returns:
            Code cell with data loading code
        """
        # Try to find data loading IR from provenance
        for activity in self.provenance.activities:
            tool_name = activity.get("type", "")
            if tool_name == "load_dataset":
                ir_dict = activity.get("ir")
                if ir_dict:
                    try:
                        ir = AnalysisStep.from_dict(ir_dict)
                        code = ir.render()
                        logger.debug("Using IR for data loading cell")
                        return new_code_cell(code)
                    except Exception as e:
                        logger.warning(f"Failed to use IR for data loading: {e}")

        # Fallback to hardcoded cell if no IR found
        logger.debug("Using fallback hardcoded data loading cell")
        code = """# Load input data
import anndata as ad

adata = ad.read_h5ad(input_data)
print(f"Loaded data: {adata.n_obs} cells Ã— {adata.n_vars} genes")
"""
        return new_code_cell(code)

    def _create_data_saving_cell(self) -> NotebookNode:
        """
        Create cell for saving output data using IR if available.

        Returns:
            Code cell with data saving code
        """
        # Try to find data saving IR from provenance
        for activity in reversed(self.provenance.activities):  # Check most recent first
            tool_name = activity.get("type", "")
            if tool_name == "save_dataset":
                ir_dict = activity.get("ir")
                if ir_dict:
                    try:
                        ir = AnalysisStep.from_dict(ir_dict)
                        code = ir.render()
                        logger.debug("Using IR for data saving cell")
                        return new_code_cell(code)
                    except Exception as e:
                        logger.warning(f"Failed to use IR for data saving: {e}")

        # Fallback to hardcoded cell if no IR found
        logger.debug("Using fallback hardcoded data saving cell")
        code = """# Save processed data
output_path = f"{output_prefix}_processed.h5ad"
adata.write_h5ad(output_path)
print(f"Saved processed data to: {output_path}")
"""
        return new_code_cell(code)

    def _create_doc_cell(
        self, activity: Dict[str, Any], step_number: int
    ) -> NotebookNode:
        """
        Create markdown documentation cell for activity.

        Args:
            activity: Activity dictionary from provenance
            step_number: Sequential step number

        Returns:
            Markdown cell documenting the analysis step
        """
        activity_type = activity.get("type", "unknown")
        params = activity.get("parameters", {})
        timestamp = activity.get("timestamp", "unknown")

        # Check if IR is present
        has_ir = activity.get("ir") is not None
        ir_status = "âœ“ IR available" if has_ir else "âš  No IR (manual review needed)"

        doc_content = f"""## Step {step_number}: {activity_type}

**Timestamp:** {timestamp}
**Agent:** {activity.get("agent", "unknown")}
**Status:** {ir_status}

"""
        if activity.get("description"):
            doc_content += f"**Description:** {activity['description']}\n\n"

        if params:
            doc_content += "**Parameters:**\n"
            for key, value in params.items():
                # Format parameter value for display
                if isinstance(value, (list, tuple)) and len(value) > 5:
                    value_str = (
                        f"[{', '.join(str(x) for x in value[:3])}...] "
                        f"(length: {len(value)})"
                    )
                else:
                    value_str = str(value)
                doc_content += f"- `{key}`: {value_str}\n"

        return new_markdown_cell(doc_content)

    def _activity_to_code(
        self, activity: Dict[str, Any], validate: bool = True
    ) -> Optional[NotebookNode]:
        """
        Convert provenance activity to executable Python code using IR.

        Args:
            activity: Activity dictionary from provenance
            validate: Whether to validate syntax

        Returns:
            Code cell with executable Python, or None if cannot generate
        """
        ir_dict = activity.get("ir")

        if ir_dict is None:
            # No IR available - return placeholder
            tool_name = activity.get("type", "unknown")
            logger.warning(f"No IR for activity: {tool_name}")

            return new_code_cell(
                f"# TODO: Manual review needed for {tool_name}\n"
                f"# No IR available - service needs to emit AnalysisStep\n"
                f"# Parameters: {activity.get('parameters', {})}\n"
                f"pass"
            )

        # Deserialize IR
        try:
            ir = AnalysisStep.from_dict(ir_dict)
        except Exception as e:
            logger.error(f"Failed to deserialize IR: {e}")
            return new_code_cell(
                f"# ERROR: Failed to deserialize IR\n" f"# {str(e)}\n" f"pass"
            )

        # Render code from template
        try:
            code = ir.render()

            # Validate syntax if requested
            if validate and ir.validates_on_export:
                try:
                    ir.validate_rendered_code()
                    logger.debug(f"Validated syntax for {ir.operation}")
                except SyntaxError as e:
                    logger.error(
                        f"Syntax error in generated code for {ir.operation}: {e}"
                    )
                    code = (
                        f"# SYNTAX ERROR in generated code:\n"
                        f"# {str(e)}\n"
                        f"# Original template:\n"
                        f"# {ir.code_template}\n"
                        f"{code}"
                    )

            return new_code_cell(code)

        except Exception as e:
            logger.error(f"Failed to render code for {ir.operation}: {e}")
            return new_code_cell(
                f"# ERROR: Template rendering failed\n"
                f"# Operation: {ir.operation}\n"
                f"# Error: {str(e)}\n"
                f"pass"
            )

    def _ir_to_code_cell(
        self, ir: AnalysisStep, validate: bool = True
    ) -> NotebookNode:
        """
        Convert AnalysisStep IR directly to executable code cell.

        Unlike _activity_to_code, this method does NOT generate placeholder
        cells - it requires a valid IR object. This ensures notebooks contain
        only executable, reproducible code.

        Args:
            ir: AnalysisStep IR object with code template
            validate: Whether to validate generated Python syntax

        Returns:
            Code cell with executable Python code

        Raises:
            ValueError: If IR rendering fails
        """
        try:
            code = ir.render()

            # Validate syntax if requested
            if validate and ir.validates_on_export:
                try:
                    ir.validate_rendered_code()
                    logger.debug(f"Validated syntax for {ir.operation}")
                except SyntaxError as e:
                    logger.error(
                        f"Syntax error in generated code for {ir.operation}: {e}"
                    )
                    code = (
                        f"# SYNTAX ERROR in generated code:\n"
                        f"# {str(e)}\n"
                        f"# Original template:\n"
                        f"# {ir.code_template}\n"
                        f"{code}"
                    )

            return new_code_cell(code)

        except Exception as e:
            logger.error(f"Failed to render code for {ir.operation}: {e}")
            raise ValueError(
                f"Failed to render IR for {ir.operation}: {e}"
            ) from e

    def _create_footer_cell(self) -> NotebookNode:
        """
        Create markdown footer cell.

        Returns:
            Markdown cell with export instructions
        """
        footer_content = """---

## Results Export

This analysis is now complete. Results have been saved with provenance tracking.

### Next Steps

1. **Review Results**: Check output files and visualizations
2. **Version Control**: Commit this notebook to Git
3. **Share**: Distribute notebook for reproducibility
4. **Re-run**: Use Papermill to execute on new data

### Usage Example

```bash
# Execute notebook with new data
papermill notebook.ipynb output.ipynb \\
    -p input_data "new_dataset.h5ad" \\
    -p output_prefix "new_analysis"
```

### IR Coverage Note

This notebook was generated using Service-Emitted IR. Steps marked with "âš  No IR"
require manual review, as the corresponding service hasn't been updated yet to emit
AnalysisStep objects. IR-enabled steps are fully reproducible.

---

*Generated with [Lobster AI](https://github.com/OmicsOS/lobster) - Multi-Agent Bioinformatics Platform*
*Using Service-Emitted IR Architecture for automatic code generation*
"""
        return new_markdown_cell(footer_content)

    def _create_provenance_summary_cell(
        self,
        total_activities: int,
        exportable_count: int,
    ) -> NotebookNode:
        """
        Create summary cell explaining provenance vs notebook content.

        This cell provides transparency about which activities are included
        in the executable notebook vs which are recorded only in provenance
        for audit trail purposes.

        Args:
            total_activities: Total number of activities in provenance
            exportable_count: Number of activities with exportable IR

        Returns:
            Markdown cell with provenance summary
        """
        filtered_count = total_activities - exportable_count

        content = f"""---

## Provenance & Reproducibility

| Metric | Count |
|--------|-------|
| **Executable Steps** | {exportable_count} |
| **Provenance-Only Activities** | {filtered_count} |
| **Total Activities** | {total_activities} |

**What's Included:**
- This notebook contains **{exportable_count} executable analysis steps** that can be reproduced
- **{filtered_count} orchestration activities** (publication processing, metadata extraction, etc.) are logged in provenance for audit trail but are not part of the executable workflow

**Full Provenance Record:**
- Session ID: `{self.provenance.namespace}`
- Complete audit trail available in provenance JSON for regulatory compliance

---
"""
        return new_markdown_cell(content)

    def _create_metadata(self, n_irs: int, n_activities: int) -> Dict[str, Any]:
        """
        Create notebook metadata with provenance link and IR stats.

        Args:
            n_irs: Number of IR objects
            n_activities: Number of activities

        Returns:
            Metadata dictionary for notebook
        """
        import os

        return {
            "source_session_id": self.provenance.namespace,
            "created_by": os.getenv("USER", "unknown"),
            "created_at": datetime.now().isoformat(),
            "lobster_version": self._get_lobster_version(),
            "dependencies": self._snapshot_dependencies(),
            "source_provenance_summary": {
                "n_activities": len(self.provenance.activities),
                "n_entities": len(self.provenance.entities),
                "n_agents": len(self.provenance.agents),
            },
            "ir_statistics": {
                "n_irs_extracted": n_irs,
                "n_activities": n_activities,
                "coverage_percent": (
                    n_irs / n_activities * 100 if n_activities > 0 else 0
                ),
            },
        }

    def _snapshot_dependencies(self) -> Dict[str, str]:
        """
        Snapshot current package versions.

        Returns:
            Dictionary of package versions
        """
        versions = {}

        # Core bioinformatics packages
        packages = ["scanpy", "anndata", "pandas", "numpy", "scipy", "scikit-learn"]

        for package in packages:
            try:
                module = __import__(package)
                versions[package] = getattr(module, "__version__", "unknown")
            except ImportError:
                pass

        # Add Python version
        versions["python"] = (
            f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        )

        return versions

    def _get_lobster_version(self) -> str:
        """Get Lobster version."""
        try:
            from lobster.version import __version__

            return __version__
        except ImportError:
            return "unknown"
